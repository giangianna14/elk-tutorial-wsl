input {
  beats {
    port => 5044
  }
}

filter {
  # Coba parse log yang datang dari Filebeat
  # Format log: [YYYY-MM-DD HH:MM:SS.mmm] LEVEL: Pesan
  grok {
    match => { "message" => "\[%{TIMESTAMP_ISO8601:log_timestamp}\] %{LOGLEVEL:log_level}: %{GREEDYDATA:log_message}" }
    overwrite => [ "message" ] # Timpa field message asli dengan log_message yang sudah diparsing
    tag_on_failure => ["_grokparsefailure_applog"] # Tambahkan tag jika parsing gagal
  }

  # Jika parsing berhasil, konversi log_timestamp ke tipe date
  if !("_grokparsefailure_applog" in [tags]) {
    date {
      match => [ "log_timestamp", "YYYY-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp" # Timpa @timestamp default dari Filebeat/Logstash dengan timestamp dari log
      remove_field => ["log_timestamp"] # Hapus field log_timestamp setelah parsing
    }

    # Mutate untuk membersihkan field atau mengubah tipe jika perlu
    mutate {
      strip => ["log_message"] # Hapus spasi di awal/akhir log_message
      # Contoh konversi jika ada field angka yang terbaca sebagai string
      # convert => { "response_time_ms" => "integer" }
    }

    # Tambahkan field ECS untuk kompatibilitas dengan fitur ML di Logs UI
    mutate {
      add_field => { "[event][dataset]" => "app.pythonlogs" }
      add_field => { "[event][module]" => "pythonapp" }
    }
  }

  # Tambahkan field tambahan jika diperlukan
  # mutate {
  #   add_field => { "application_name" => "my-python-app" }
  # }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    action => "create" # Diperlukan untuk data streams
    data_stream => "true"
    data_stream_type => "logs"
    data_stream_dataset => "app" # Sesuaikan dengan dataset Anda, misal "app.pythonlogs"
    data_stream_namespace => "default" # Sesuaikan jika perlu
    # user => "elastic" # Jika Elasticsearch Anda menggunakan security
    # password => "password"
  }

  # Untuk debugging, Anda juga bisa output ke stdout di terminal Logstash
  # stdout { codec => rubydebug }
}
